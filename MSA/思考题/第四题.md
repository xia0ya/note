# 矩阵求导，标量函数的偏导数矩阵

---

$$f(\alpha_1, \lambda) = \alpha_1^T \Sigma \alpha_1 + \lambda (\alpha_1^T \alpha_1 - 1)$$


$$
\begin{cases}
\frac{\partial f}{\partial \alpha_1} = 0 \\
\frac{\partial f}{\partial \lambda} = 0
\end{cases}
\Rightarrow \cdots
$$
思考题：

矩阵求导，标量函数的偏导数矩阵
## 求解方法一

$$tr(AB^T) = tr(B^T A) = tr(A^T B) = tr(BA^T)$$

$$d(AXB) = Ad(X)B$$

$$d|X| =|X| tr(X^T d(X))$$

$$d(X^{-1}) = -X^{-1} d(X) X^{-1}$$
---
$$f(\alpha_1, \lambda) = \alpha_1^T \Sigma \alpha_1 + \lambda (\alpha_1^T \alpha_1 - 1)$$

$$\frac{\partial f}{\partial \alpha_1} = ?$$

① $$d (\alpha_1^T \Sigma \alpha_1 + \lambda \alpha_1^T \alpha_1 - \lambda E)$$

$$= tr(d (\alpha_1^T \Sigma \alpha_1 + \lambda \alpha_1^T \alpha_1) + \lambda d (\alpha_1^T \alpha_1))$$

$$= tr(\Sigma \alpha_1 d(\alpha_1^T) + \alpha_1^T \Sigma d(\alpha_1)) + \lambda \alpha_1^T d(\alpha_1) + \lambda d(\alpha_1^T) \alpha_1$$

$$= tr(\alpha_1^T \Sigma^T d(\alpha_1) + \alpha_1^T \Sigma d(\alpha_1)) + \lambda \alpha_1^T d(\alpha_1) + \lambda d(\alpha_1^T) \alpha_1$$

$$\frac{\partial f}{\partial \alpha_1^T} = \alpha_1^T \Sigma^T + \alpha_1^T \Sigma + \lambda \alpha_1^T + \lambda \alpha_1^T$$

$$\frac{\partial f}{\partial \alpha_1} = \Sigma \alpha_1 + \Sigma^T \alpha_1 + 2 \lambda \alpha_1 = 0$$

此处，若加号两边相等，则问题得解。

---
Two Way ：以一维推高维

$$f(\lambda, \alpha_1) = \alpha_1^T \Sigma \alpha_1 - \lambda (\alpha_1^T \alpha_1 - 1)$$

$$\frac{\partial f}{\partial \alpha_1} = 2 \Sigma \alpha_1 - 2 \lambda \alpha_1 = 0$$

即 $$\Sigma \alpha_1 = \lambda \alpha_1$$

$$\alpha_1 \text{ 是 } \Sigma \text{ 的特征向量}$$

$$Var(y_{i1}) = \lambda_1$$

最大方差y1, 最大特征值

---
# 方差，协方差相关
$$Var(y_1) = \alpha_1^T \Sigma \alpha_1$$
$$= \alpha_1^T \lambda_1 \alpha_1$$
$$= \lambda_1 \alpha_1^T \alpha_1$$
$$= \lambda_1 \quad ||\alpha_1||=1$$

$$Cov(y_1, y_2) = \alpha_1^T \Sigma \alpha_2$$
$$= \alpha_1^T \lambda_2 \alpha_2$$
$$= \lambda_2 \alpha_1^T \alpha_2$$
$$= 0$$

特征值，特征向量定义相关，$\alpha_1$是从属于$\lambda_1$的特征向量

实对称矩阵对应不同特征值的特征向量之间正交(即在统计上不相关)。但是如果有重根，施密特正交化可以找到两个正交的向量

## 使用相关系数提取主成分
$R$
$$
Tr(R)=P
$$

$$
\frac{\lambda_j}{P}>\frac{1}{P}
$$
即
$$
\lambda_j>1
$$

## $\rho (x,y)$
$$\rho_{Y_1, X_i} = \frac{e_{1i}\sqrt{\lambda_1}}{\sqrt{\sigma_{ii}}} = \frac{\text{Cov}(Y_1, X_i)}{\sqrt{\text{Var}(Y_1)}\sqrt{\text{Var}(X_i)}}$$

法一：
$$\text{Cov}(Y_1, X_1) \quad X \sim (0, \Sigma) \quad \text{-->假设}$$
$$= \text{Cov}(a_1^T X, X_1) \quad X = (X_1, \cdots, X_p)^T$$


$$= e_{11}\text{Cov}(X_1, X_1) + \cdots +e_{1p}\text{Cov}(X_p, X_1) \quad \text{求解不方便}$$


法二：记 $$\beta_1 = (1, 0, \cdots, 0)^T, X_1 = \beta_1^T X$$
**这是关键**
$$\text{Cov}(Y_1, X_1) = \text{Cov}(a_1^T X, \beta_1^T X)$$
$$= a_1^T \Sigma \beta_1 = \beta_1^T \Sigma a_1 = \lambda_1 \beta_1^T a_1 = \lambda_1 e_{11}$$

$\alpha_1$ 第一个特征值对应的特征向量.

